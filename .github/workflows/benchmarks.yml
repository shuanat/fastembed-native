# ============================================================================
# Performance Benchmarks Workflow
# ============================================================================
#
# Purpose:
#   Run performance benchmarks across all platforms and language bindings
#   to verify performance characteristics before release
#
# Trigger:
#   - Manual trigger via workflow_dispatch
#   - On release branches (release/*)
#
# Platforms:
#   - Linux (x64) - Ubuntu latest
#   - Windows (x64) - Windows latest
#   - macOS (x64/arm64) - macOS latest
#
# ============================================================================

name: Performance Benchmarks

on:
  workflow_dispatch:
  push:
    branches: [release/*]
    paths:
      - "bindings/**"
      - "tests/benchmark*.c"
      - ".github/workflows/benchmarks.yml"

env:
  ONNX_VERSION: "1.23.2"

jobs:
  benchmark-linux:
    name: Benchmarks (Linux)
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y nasm gcc make

      - name: Build shared library
        run: |
          cd bindings/shared
          make all
          make shared

      - name: Build C benchmark
        run: |
          cd tests
          gcc -O2 -I../bindings/shared/include benchmark_improved.c \
            -L../bindings/shared/build -lfastembed_native -lm -o benchmark
          chmod +x benchmark

      - name: Run C benchmark
        run: |
          cd tests
          export LD_LIBRARY_PATH=../bindings/shared/build:$LD_LIBRARY_PATH
          ./benchmark > benchmark_results_linux.txt 2>&1 || true
          cat benchmark_results_linux.txt

      - name: Build Node.js binding
        run: |
          cd bindings/nodejs
          npm install
          npm run build

      - name: Run Node.js benchmark
        run: |
          cd bindings/nodejs
          node benchmark.js > benchmark_nodejs_linux.txt 2>&1 || true
          cat benchmark_nodejs_linux.txt

      - name: Build Python binding
        run: |
          cd bindings/python
          python3 -m pip install --upgrade pip
          python3 setup.py build_ext --inplace

      - name: Run Python benchmark
        run: |
          cd bindings/python
          export LD_LIBRARY_PATH=../shared/build:$LD_LIBRARY_PATH
          python3 benchmark.py > benchmark_python_linux.txt 2>&1 || true
          cat benchmark_python_linux.txt

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-linux
          path: |
            tests/benchmark_results_linux.txt
            bindings/nodejs/benchmark_nodejs_linux.txt
            bindings/python/benchmark_python_linux.txt
          retention-days: 30

  benchmark-windows:
    name: Benchmarks (Windows)
    runs-on: windows-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install NASM
        run: |
          choco install nasm -y

      - name: Build shared library
        run: |
          cd bindings/shared
          .\scripts\build_windows.ps1

      - name: Build C benchmark
        shell: pwsh
        run: |
          cd tests
          cl /O2 /I..\bindings\shared\include benchmark_improved.c \
            /link /LIBPATH:..\bindings\shared\build fastembed_native.lib /OUT:benchmark.exe
          if ($LASTEXITCODE -ne 0) { Write-Output "Build failed, continuing..." }

      - name: Run C benchmark
        shell: pwsh
        run: |
          cd tests
          $env:PATH = "..\bindings\shared\build;$env:PATH"
          if (Test-Path "benchmark.exe") {
            .\benchmark.exe > benchmark_results_windows.txt 2>&1
            Get-Content benchmark_results_windows.txt
          } else {
            Write-Output "Benchmark executable not found, skipping..."
          }

      - name: Build Node.js binding
        run: |
          cd bindings/nodejs
          npm install
          npm run build

      - name: Run Node.js benchmark
        run: |
          cd bindings/nodejs
          node benchmark.js > benchmark_nodejs_windows.txt 2>&1 || echo "Benchmark failed"
          type benchmark_nodejs_windows.txt

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-windows
          path: |
            tests/benchmark_results_windows.txt
            bindings/nodejs/benchmark_nodejs_windows.txt
          retention-days: 30

  benchmark-macos-arm64:
    name: Benchmarks (macOS ARM64)
    runs-on: macos-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup architecture
        run: |
          echo "ARCH=arm64" >> $GITHUB_ENV
          echo "Building for Apple Silicon (ARM64)"

      - name: Install dependencies
        run: |
          brew install nasm || true

      - name: Build shared library
        run: |
          cd bindings/shared
          make all
          make shared

      - name: Build C benchmark
        run: |
          cd tests
          gcc -O2 -I../bindings/shared/include benchmark_improved.c \
            -L../bindings/shared/build -lfastembed_native -lm -o benchmark
          chmod +x benchmark

      - name: Run C benchmark
        run: |
          cd tests
          export DYLD_LIBRARY_PATH=../bindings/shared/build:$DYLD_LIBRARY_PATH
          ./benchmark > benchmark_results_macos_arm64.txt 2>&1 || true
          cat benchmark_results_macos_arm64.txt

      - name: Build Node.js binding
        run: |
          cd bindings/nodejs
          npm install
          npm run build

      - name: Run Node.js benchmark
        run: |
          cd bindings/nodejs
          node benchmark.js > benchmark_nodejs_macos_arm64.txt 2>&1 || true
          cat benchmark_nodejs_macos_arm64.txt

      - name: Build Python binding
        run: |
          cd bindings/python
          python3 -m pip install --upgrade pip
          python3 setup.py build_ext --inplace

      - name: Run Python benchmark
        run: |
          cd bindings/python
          export DYLD_LIBRARY_PATH=../shared/build:$DYLD_LIBRARY_PATH
          python3 benchmark.py > benchmark_python_macos_arm64.txt 2>&1 || true
          cat benchmark_python_macos_arm64.txt

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-macos-arm64
          path: |
            tests/benchmark_results_macos_arm64.txt
            bindings/nodejs/benchmark_nodejs_macos_arm64.txt
            bindings/python/benchmark_python_macos_arm64.txt
          retention-days: 30

  aggregate-results:
    name: Aggregate Benchmark Results
    needs: [benchmark-linux, benchmark-windows, benchmark-macos-arm64]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: benchmark-results

      - name: Aggregate results
        run: |
          cat > aggregate_benchmarks.sh << 'EOF'
          #!/bin/bash

          # Function to extract benchmark data from text file
          extract_benchmark_data() {
            local file="$1"
            local platform="$2"
            
            if [ ! -f "$file" ]; then
              return
            fi
            
            # Extract dimension, avg time, and throughput from table format
            # Format: "  DIM    | AVG_TIME    | THROUGHPUT"
            grep -E "^\s+[0-9]+\s+\|\s+[0-9]+\.[0-9]+\s+\|\s+[0-9]+" "$file" | while read line; do
              dimension=$(echo "$line" | awk -F'|' '{print $1}' | xargs)
              avg_time=$(echo "$line" | awk -F'|' '{print $2}' | xargs)
              throughput=$(echo "$line" | awk -F'|' '{print $3}' | xargs)
              
              # Get text label from context (look for "=== Benchmark:" line before)
              text_label=$(grep -B 5 "$line" "$file" | grep "=== Benchmark:" | head -1 | sed 's/=== Benchmark: //' | sed 's/ ===//' | xargs)
              
              if [ -n "$dimension" ] && [ -n "$avg_time" ] && [ -n "$throughput" ]; then
                echo "${platform}|${text_label}|${dimension}|${avg_time}|${throughput}"
              fi
            done
          }

          # Output file
          OUTPUT="BENCHMARK_RESULTS_CI.md"

          # Header
          echo "# Benchmark Results - $(date +%Y-%m-%d)" > "$OUTPUT"
          echo "" >> "$OUTPUT"
          echo "## Summary" >> "$OUTPUT"
          echo "" >> "$OUTPUT"
          echo "Performance benchmarks across all platforms and language bindings." >> "$OUTPUT"
          echo "" >> "$OUTPUT"

          # Extract data from all platforms
          TEMP_DATA=$(mktemp)

          # Linux
          for file in benchmark-results/benchmark-results-linux/*.txt; do
            extract_benchmark_data "$file" "Linux x64" >> "$TEMP_DATA"
          done

          # Windows
          for file in benchmark-results/benchmark-results-windows/*.txt; do
            extract_benchmark_data "$file" "Windows x64" >> "$TEMP_DATA"
          done

          # macOS ARM64
          for file in benchmark-results/benchmark-results-macos-arm64/*.txt; do
            extract_benchmark_data "$file" "macOS ARM64" >> "$TEMP_DATA"
          done

          # Generate tables by text length
          for text_type in "Short (~5 chars)" "Medium (~40 chars)" "Long (~250 chars)"; do
            echo "### $text_type" >> "$OUTPUT"
            echo "" >> "$OUTPUT"
            echo "| Platform | Dimension | Avg Time (ms) | Throughput (emb/s) |" >> "$OUTPUT"
            echo "|----------|----------|---------------|---------------------|" >> "$OUTPUT"
            
            grep "|${text_type}|" "$TEMP_DATA" | sort -t'|' -k3 -n | while IFS='|' read -r platform label dimension avg_time throughput; do
              printf "| %s | %s | %.4f | %.0f |\n" "$platform" "$dimension" "$avg_time" "$throughput" >> "$OUTPUT"
            done
            
            echo "" >> "$OUTPUT"
          done

          # Overall comparison table
          echo "## Overall Performance Comparison" >> "$OUTPUT"
          echo "" >> "$OUTPUT"
          echo "| Platform | 128D (ms) | 768D (ms) | Best Throughput (emb/s) |" >> "$OUTPUT"
          echo "|----------|-----------|-----------|-------------------------|" >> "$OUTPUT"

          for platform in "Linux x64" "Windows x64" "macOS ARM64"; do
            dim128=$(grep "^${platform}|" "$TEMP_DATA" | grep "|128|" | head -1 | cut -d'|' -f4)
            dim768=$(grep "^${platform}|" "$TEMP_DATA" | grep "|768|" | head -1 | cut -d'|' -f4)
            best_throughput=$(grep "^${platform}|" "$TEMP_DATA" | cut -d'|' -f5 | sort -n | tail -1)
            
            if [ -n "$dim128" ] && [ -n "$dim768" ] && [ -n "$best_throughput" ]; then
              printf "| %s | %.4f | %.4f | %.0f |\n" "$platform" "$dim128" "$dim768" "$best_throughput" >> "$OUTPUT"
            fi
          done

          echo "" >> "$OUTPUT"
          echo "## Raw Results" >> "$OUTPUT"
          echo "" >> "$OUTPUT"
          echo "### Linux x64" >> "$OUTPUT"
          echo '```' >> "$OUTPUT"
          cat benchmark-results/benchmark-results-linux/*.txt >> "$OUTPUT" 2>/dev/null || echo "No results" >> "$OUTPUT"
          echo '```' >> "$OUTPUT"
          echo "" >> "$OUTPUT"
          echo "### Windows x64" >> "$OUTPUT"
          echo '```' >> "$OUTPUT"
          cat benchmark-results/benchmark-results-windows/*.txt >> "$OUTPUT" 2>/dev/null || echo "No results" >> "$OUTPUT"
          echo '```' >> "$OUTPUT"
          echo "" >> "$OUTPUT"
          echo "### macOS ARM64 (Apple Silicon)" >> "$OUTPUT"
          echo '```' >> "$OUTPUT"
          cat benchmark-results/benchmark-results-macos-arm64/*.txt >> "$OUTPUT" 2>/dev/null || echo "No results" >> "$OUTPUT"
          echo '```' >> "$OUTPUT"

          # Cleanup
          rm -f "$TEMP_DATA"

          # Display result
          cat "$OUTPUT"
          EOF

          chmod +x aggregate_benchmarks.sh
          ./aggregate_benchmarks.sh

      - name: Upload aggregated results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-aggregated
          path: BENCHMARK_RESULTS_CI.md
          retention-days: 30
